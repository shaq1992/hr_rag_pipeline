<ingestion/Dockerfile>
FROM python:3.11-slim

WORKDIR /app

COPY requirements.txt .

# Force the CPU-only version of PyTorch to avoid a 2.5GB CUDA download
RUN pip install torch --index-url https://download.pytorch.org/whl/cpu && \
    pip install --no-cache-dir -r requirements.txt

</ingestion/Dockerfile>

<ingestion/docker-compose.yml>
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  ingestion_api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - .:/app
      - ../sources:/app/sources  # Direct access to the local WSL sources directory
      - huggingface_cache:/root/.cache/huggingface
    env_file:
      - .env
    depends_on:
      - qdrant
    command: uvicorn main:app --host 0.0.0.0 --port 8000 --reload
volumes:
  huggingface_cache:

</ingestion/docker-compose.yml>

<ingestion/infra/embedding_utils.py>
from FlagEmbedding import BGEM3FlagModel
import logging

logger = logging.getLogger(__name__)

# Global variable to hold the model singleton
_model = None

def get_model():
    """Lazy loads the model only when first requested."""
    global _model
    if _model is None:
        logger.info("Downloading/Loading BGE-M3 model weights... (This may take a few minutes on the first run)")
        # use_fp16=False ensures compatibility for CPU execution
        _model = BGEM3FlagModel('BAAI/bge-m3', use_fp16=False)
        logger.info("BGE-M3 loaded successfully.")
    return _model

def get_bge_m3_embeddings(text: str):
    """Generates Dense (1024d) and Sparse (Lexical) embeddings in a single pass."""
    model = get_model()
    
    output = model.encode([text], return_dense=True, return_sparse=True, return_colbert_vecs=False)
    
    dense_vec = output['dense_vecs'][0].tolist()
    lexical_weights = output['lexical_weights'][0]
    
    # Qdrant requires sparse indices to be integers and values to be floats
    sparse_indices = [int(k) for k in lexical_weights.keys()]
    sparse_values = [float(v) for v in lexical_weights.values()]
    
    return dense_vec, sparse_indices, sparse_values

</ingestion/infra/embedding_utils.py>

<ingestion/infra/llm_response.py>
from google import genai
from dotenv import load_dotenv
import os

# Load environment variables from .env file
load_dotenv()


def get_response(prompt):
    API_KEY = os.getenv("GEMINI_API_KEY")
    client = genai.Client(api_key = API_KEY)

    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents= prompt
    )

    return response.text

if __name__ == "__main__":

    prompt= "hi can you hear"
    response= get_response(prompt)
    print(response)

</ingestion/infra/llm_response.py>

<ingestion/infra/llm_utils.py>
from google import genai
import os

client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))

def summarize_table(raw_html: str):
    """Generates a dense semantic summary of a table's rules and structure."""
    prompt = f"Provide a detailed semantic summary of the following HR policy table's purpose, rules, and relationships so it can be vector searched accurately:\n\n{raw_html}"
    
    response = client.models.generate_content(
        model='gemini-2.5-flash',
        contents=prompt,
        config={'temperature': 0.1}
    )
    return response.text, response.usage_metadata

</ingestion/infra/llm_utils.py>

<ingestion/infra/qdrant_utils.py>
from qdrant_client import QdrantClient, models
import os

QDRANT_HOST = os.getenv("QDRANT_HOST", "qdrant")
client = QdrantClient(host=QDRANT_HOST, port=6333)

def init_collection(collection_name: str):
    """Initializes the collection with dense and sparse vector configurations and payload indexes."""
    if not client.collection_exists(collection_name):
        client.create_collection(
            collection_name=collection_name,
            vectors_config={
                "dense": models.VectorParams(size=1024, distance=models.Distance.COSINE)
            },
            sparse_vectors_config={
                "sparse": models.SparseVectorParams()
            }
        )
        
        # Hardware-accelerated payload filtering
        client.create_payload_index(collection_name, "source_document", field_schema="keyword")
        client.create_payload_index(collection_name, "sub_section", field_schema="keyword")

def upsert_chunk(collection_name: str, point_id: str, dense: list, sparse_idx: list, sparse_val: list, payload: dict):
    """Upserts a hybrid point into Qdrant."""
    client.upsert(
        collection_name=collection_name,
        points=[
            models.PointStruct(
                id=point_id,
                vector={
                    "dense": dense,
                    "sparse": models.SparseVector(indices=sparse_idx, values=sparse_val)
                },
                payload=payload
            )
        ]
    )

</ingestion/infra/qdrant_utils.py>

<ingestion/main.py>
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
import uuid
import os
import logging
import json

from unstructured_client import UnstructuredClient
from unstructured_client.models import operations, shared

from infra.llm_utils import summarize_table
from infra.embedding_utils import get_bge_m3_embeddings
from infra.qdrant_utils import init_collection, upsert_chunk

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

app = FastAPI(title="HR Knowledge Base Ingestion Pipeline")
COLLECTION_NAME = "hr_policies"

# Initialize the Unstructured Serverless Client
unstructured_client = UnstructuredClient(
    api_key_auth=os.getenv("UNSTRUCTURED_API_KEY"),
    server_url=os.getenv("UNSTRUCTURED_ENDPOINT"),
)

@app.on_event("startup")
def startup_event():
    logger.info("Initializing Qdrant Collection...")
    init_collection(COLLECTION_NAME)

class IngestRequest(BaseModel):
    file_path: str

@app.post("/ingest")
def ingest_document(request: IngestRequest):
    if not os.path.exists(request.file_path):
        raise HTTPException(status_code=404, detail="File not found")
    
    file_name = os.path.basename(request.file_path)
    
    def process_stream():
        logger.info(f"Starting API ingestion for {file_name}")
        yield json.dumps({"status": "init", "message": f"Starting {file_name}..."}) + "\n"
        
        llm_calls = 0
        prompt_tokens = 0
        candidate_tokens = 0
        
        def update_telemetry(usage):
            nonlocal llm_calls, prompt_tokens, candidate_tokens
            llm_calls += 1
            if usage:
                prompt_tokens += getattr(usage, 'prompt_token_count', 0)
                candidate_tokens += getattr(usage, 'candidates_token_count', 0)

        try:
            yield json.dumps({"status": "parsing", "message": "Calling Unstructured API..."}) + "\n"
            with open(request.file_path, "rb") as f:
                req = operations.PartitionRequest(
                    partition_parameters=shared.PartitionParameters(
                        files=shared.Files(content=f.read(), file_name=file_name),
                        strategy=shared.Strategy.HI_RES,
                    )
                )
                res = unstructured_client.general.partition(request=req)
                elements = res.elements
            
            # Strip structural noise
            filtered_elements = [e for e in elements if e.get("type") not in ["Header", "Footer"]]
            current_section_header = "General Policy"
            chunks_upserted = 0
            
            yield json.dumps({"status": "start_chunks", "total": len(filtered_elements)}) + "\n"
            
            for idx, element in enumerate(filtered_elements):
                chunk_text = str(element.get("text", "")).strip()
                if not chunk_text:
                    continue
                    
                content_type = "text"
                raw_content = chunk_text
                embed_text = chunk_text
                
                if element.get("type") == "Title":
                    current_section_header = chunk_text
                    yield json.dumps({"status": "chunk_progress", "current": idx + 1}) + "\n"
                    continue
                    
                elif element.get("type") == "Table":
                    content_type = "table"
                    metadata = element.get("metadata", {})
                    raw_content = metadata.get("text_as_html", chunk_text)
                    
                    # Only call Gemini for table elements
                    embed_text, usage = summarize_table(raw_content)
                    update_telemetry(usage)
                
                # Extract page number dynamically
                page_number = element.get("metadata", {}).get("page_number", "Unknown")
                
                # Generate vectors
                dense, s_idx, s_val = get_bge_m3_embeddings(embed_text)
                
                # Leaner Payload Construction
                payload = {
                    "source_document": file_name,
                    "page_number": page_number,
                    "section_header": current_section_header,
                    "content_type": content_type,
                    "raw_content": raw_content,
                    "chunk_index": idx
                }
                
                point_id = str(uuid.uuid4())
                upsert_chunk(COLLECTION_NAME, point_id, dense, s_idx, s_val, payload)
                chunks_upserted += 1
                
                yield json.dumps({"status": "chunk_progress", "current": idx + 1}) + "\n"
                
            logger.info(f"Successfully processed {file_name}.")
            yield json.dumps({
                "status": "success", 
                "file": file_name, 
                "chunks_upserted": chunks_upserted,
                "llm_calls": llm_calls
            }) + "\n"
            
        except Exception as e:
            logger.error(f"Ingestion failed: {str(e)}")
            yield json.dumps({"status": "error", "detail": str(e)}) + "\n"

    return StreamingResponse(process_stream(), media_type="application/x-ndjson")

</ingestion/main.py>

