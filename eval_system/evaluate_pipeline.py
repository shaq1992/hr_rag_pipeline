import json
import asyncio
import httpx
import time
import os
import logging
from pydantic import BaseModel
from google import genai
from google.genai import types

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
logger = logging.getLogger(__name__)

# Environment Configuration
API_URL = "http://rag_api:8080/query"
LOG_FILE = "/app/logs/rag_events.jsonl"
API_KEY = os.getenv("GEMINI_API_KEY")

class GenMetrics(BaseModel):
    answer_correctness: float  # 0.0 to 1.0
    hallucination_score: float # 0.0 (hallucinated) to 1.0 (strictly grounded)
    citation_accuracy: float   # 0.0 to 1.0
    reasoning: str

class Evaluator:
    def __init__(self):
        self.gemini_client = genai.Client(api_key=API_KEY)
        self.http_client = httpx.AsyncClient(timeout=60.0)

    async def run_test_case(self, tc: dict) -> dict:
        query = tc["query"]
        expected_source = tc["expected_source"]
        expected_facts = tc["expected_facts"]
        
        logger.info(f"Executing TC: {tc['id']} - {query}")
        
        # 1. Fire the query at the API Gateway
        response = await self.http_client.post(API_URL, json={"query": query})
        actual_answer = response.text
        
        # 2. Allow disk I/O to flush the JSONL log from the API Gateway
        time.sleep(1.5)
        
        # 3. Read the exact internal state from the log file
        log_entry = self._get_latest_log_entry(query)
        retrieved_chunks = log_entry.get("retrieved_chunks", []) if log_entry else []
        
        # 4. Calculate Deterministic Retrieval Metrics
        recall, mrr = self._calculate_retrieval_metrics(retrieved_chunks, expected_source)
        
        # 5. Calculate Probabilistic Generation Metrics using LLM-as-a-Judge
        gen_metrics = await self._evaluate_generation(query, expected_facts, retrieved_chunks, actual_answer, expected_source)
        
        return {
            "test_id": tc["id"],
            "difficulty": tc["difficulty"],
            "recall_at_k": recall,
            "mrr": mrr,
            "answer_correctness": gen_metrics.answer_correctness,
            "hallucination_score": gen_metrics.hallucination_score,
            "citation_accuracy": gen_metrics.citation_accuracy,
            "judge_reasoning": gen_metrics.reasoning
        }

    def _get_latest_log_entry(self, query: str):
        """Reads the shared volume log file backwards to find the telemetry for this query."""
        if not os.path.exists(LOG_FILE):
            return None
        with open(LOG_FILE, 'r') as f:
            lines = f.readlines()
            for line in reversed(lines):
                try:
                    data = json.loads(line)
                    if data.get("query") == query:
                        return data
                except json.JSONDecodeError:
                    continue
        return None

    def _calculate_retrieval_metrics(self, chunks: list, expected_source: str):
        """Calculates Recall@K and Mean Reciprocal Rank (MRR)."""
        if expected_source == "None":
            # Edge case: Out of scope queries shouldn't retrieve specific sources
            return (1.0, 1.0) if not chunks else (0.0, 0.0)

        for rank, chunk in enumerate(chunks):
            if expected_source in chunk.get("source", ""):
                # Found the document!
                return 1.0, 1.0 / (rank + 1)
        
        return 0.0, 0.0

    async def _evaluate_generation(self, query: str, expected_facts: list, chunks: list, answer: str, expected_source: str) -> GenMetrics:
        """Uses Gemini 2.5 Flash to act as an objective judge of the generated response."""
        context_str = "\n".join([c.get("content", "") for c in chunks])
        
        prompt = f"""
        You are an impartial AI evaluator grading a RAG system.
        
        User Query: {query}
        Expected Facts to be Present: {expected_facts}
        Expected Source Document: {expected_source}
        
        Context Retrieved by System: {context_str}
        
        Actual Answer Generated by System: {answer}
        
        Evaluate the Actual Answer and output JSON with the following scores (0.0 to 1.0):
        1. answer_correctness: Does the answer address the query and contain the expected facts?
        2. hallucination_score: Is the answer STRICTLY derived from the retrieved context? (1.0 = purely grounded, 0.0 = completely hallucinated). If no context was retrieved and the system gracefully admitted it, score 1.0.
        3. citation_accuracy: Did the answer explicitly cite the '{expected_source}' document? (Score 1.0 if cited or if expected_source is 'None' and it declined to answer).
        """
        
        try:
            response = await self.gemini_client.aio.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema=GenMetrics,
                    temperature=0.0,
                ),
            )
            return response.parsed
        except Exception as e:
            logger.error(f"LLM Judge failed: {e}")
            return GenMetrics(answer_correctness=0.0, hallucination_score=0.0, citation_accuracy=0.0, reasoning=f"Error: {e}")

async def main():
    logger.info("Initializing Evaluation Pipeline...")
    with open("test_cases.json", "r") as f:
        test_cases = json.load(f)
        
    evaluator = Evaluator()
    results = []
    
    for tc in test_cases:
        res = await evaluator.run_test_case(tc)
        results.append(res)
        
    await evaluator.http_client.aclose()
    
    # Print Beautiful Summary
    print("\n" + "="*80)
    print(f"{'EVALUATION REPORT':^80}")
    print("="*80)
    
    avg_recall = sum(r["recall_at_k"] for r in results) / len(results)
    avg_mrr = sum(r["mrr"] for r in results) / len(results)
    avg_correct = sum(r["answer_correctness"] for r in results) / len(results)
    avg_hallucination = sum(r["hallucination_score"] for r in results) / len(results)
    
    for r in results:
        print(f"Test Case: {r['test_id']} ({r['difficulty'].upper()})")
        print(f"  Retrieval -> Recall@K: {r['recall_at_k']:.2f} | MRR: {r['mrr']:.2f}")
        print(f"  Generation-> Correctness: {r['answer_correctness']:.2f} | Grounding: {r['hallucination_score']:.2f} | Citation: {r['citation_accuracy']:.2f}")
        print(f"  Judge Note: {r['judge_reasoning']}\n")
        
    print("="*80)
    print(f"SYSTEM AGGREGATE METRICS:")
    print(f"Mean Recall@K:         {avg_recall:.2f}")
    print(f"Mean Reciprocal Rank:  {avg_mrr:.2f}")
    print(f"Mean Correctness:      {avg_correct:.2f}")
    print(f"Mean Grounding (Anti-Hallucination): {avg_hallucination:.2f}")
    print("="*80)

    # Save to disk
    with open("evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2)

if __name__ == "__main__":
    asyncio.run(main())
